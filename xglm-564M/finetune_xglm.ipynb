{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29yOcSxnBvlk"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install gdown\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from typing import Union,List\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "from transformers import XGLMTokenizer, XGLMForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "from datasets import load_dataset\n"
      ],
      "metadata": {
        "id": "W8JDtdTRB0rL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(base_model:str=\"facebook/xglm-564M\"):\n",
        "    tokenizer = XGLMTokenizer.from_pretrained(base_model)\n",
        "    model = XGLMForCausalLM.from_pretrained(base_model)\n",
        "    return model,tokenizer\n",
        "model,tokenizer = load_model()"
      ],
      "metadata": {
        "id": "Lrtg22NwCC6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_tokens = ['<human>:', '<bot>:']\n",
        "\n",
        "tokenizer.add_tokens(list(new_tokens))\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "YoyXbmmQqAW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "url = 'https://drive.google.com/uc?export=download&id=1jbbUtwgwoSQgGnXxzTh-nMReVzEU7ZTU&confirm=t&uuid=d79e2e78-51de-466f-9ceb-3944606141a2&at=AKKF8vwcgi95TGSnSQUNCKx4NTqS:1682865249145'\n",
        "output = 'output.jsonl'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "id": "I7gxeJn7CHUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(prompt):\n",
        "    return {'prompt':f\"{prompt['Background:']} <human>: {prompt['<human>:']} <bot>: {prompt['<bot>:']}\"}\n",
        "# format data like <sep> context <human>...<bot>...<sep>\n",
        "def preprocess(prompt):\n",
        "    data = tokenizer(\n",
        "        prompt['prompt'],\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        padding=False,\n",
        "        return_tensors=None,\n",
        "    )\n",
        "    data['input_ids'].append(tokenizer.eos_token_id)\n",
        "    data['attention_mask'].append(1)\n",
        "    data['labels'] = data['input_ids'].copy()\n",
        "    return data"
      ],
      "metadata": {
        "id": "DH0JK5NID4qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "datasets = load_dataset('json',data_files = 'output.jsonl')\n",
        "datasets"
      ],
      "metadata": {
        "id": "-kUNjqBPDxyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = datasets['train']\n",
        "datasets = datasets.map(format_prompt,remove_columns=['Background:', '<human>:', '<bot>:'])\n",
        "datasets = datasets.map(lambda x:{'token':len(tokenizer.tokenize(x['prompt']))})\n",
        "datasets = datasets.filter(lambda x:x['token']<255)\n",
        "datasets = datasets.map(preprocess,remove_columns=['prompt','token']) \n",
        "datasets"
      ],
      "metadata": {
        "id": "NPYyqRNnG7_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = datasets.filter(lambda x:x['input_ids'][0] == 2)\n",
        "datasets = datasets.filter(lambda x:x['input_ids'][-1] == 2)  \n",
        "datasets"
      ],
      "metadata": {
        "id": "G-5Opy41qT2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U7D2cstUPbyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "micro_batch_size = 4\n",
        "gradient_accumulation_steps = batch_size // micro_batch_size\n",
        "num_epochs = 3\n",
        "learning_rate = 3e-7\n",
        "output_dir = 'checkpoint-xglm'"
      ],
      "metadata": {
        "id": "k3s_B_zsqzf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_PROJECT\"] = 'wandb_project'\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = 'true'"
      ],
      "metadata": {
        "id": "ntzcUSCZyVg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_args = TrainingArguments( # สร้าง class train-args\n",
        "            per_device_train_batch_size=micro_batch_size, # btch_size \n",
        "            gradient_accumulation_steps=gradient_accumulation_steps, # https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation เหมือนจะ ค่อยๆคำนวนค่า gradient ตามค่าที่ใส่เข้าไปรอบ แล้วค่อยปรับ weight ทีเดียว ไม่รู้_\n",
        "            gradient_checkpointing=True,\n",
        "            warmup_steps=100,# ไม่รู้_\n",
        "            num_train_epochs=num_epochs, # จำนวน epoch\n",
        "            learning_rate=learning_rate,# ค่า learning-rate\n",
        "            fp16= True, # ไม่รู้ว่าคืออะไร ตอนแรก default คือ True เลยเปลี่ยนเป็น False แทน แล้วรันได้เฉย _   ///////// เพราะเราเซ้ตข้างบนไว้ว่าเป็น torch.float16 \n",
        "            logging_steps=1, # ไม่รู้_ //////////// แสดงผลตอนเทรนทุกๆ 10 step gradient descent\n",
        "            optim=\"adamw_torch\",# ชื่อ optimizer มั้ง_ /////// yes!!\n",
        "            save_strategy=\"steps\", # ไม่รู้_ //////////////////////////////// save model based on epoch? steps?\n",
        "            save_steps=1_000, # ไม่รู้_ ///////////////////// Save model every 200 optimizer.step()\n",
        "            output_dir=output_dir, # ไม่รู้_ ////////////////////////// Where to save model\n",
        "            save_total_limit=3, # ไม่รู้_ /////////////////////////// Limit model save amount (Not to have 300 model file when you train 300 epoch)\n",
        "            report_to=\"wandb\", # ใช้ wandb\n",
        "            run_name='finetune-xglm', # ชื่อ task\n",
        "        )\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "      model=model,# model ที่จะเอาไปเทรน\n",
        "      train_dataset=datasets, # data ใน train-set\n",
        "      # eval_dataset=val_data,\n",
        "      args=train_args,\n",
        "      data_collator=DataCollatorForSeq2Seq(\n",
        "          tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True \n",
        "      ),\n",
        "  )\n",
        "\n",
        "model.config.use_cache = False\n",
        "\n",
        "# train-ai ปกติ\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "-snNulttqcrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "29z9ohVVuuzB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}