{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q accelerate==0.18.0\n!pip install -q datasets\n!pip install -q transformers\n!pip install -q sentencepiece\n!pip install -q gdown\n!pip install -q wandb","metadata":{"id":"29yOcSxnBvlk","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport os\nfrom typing import Union,List\nimport sys\n\nimport torch\nfrom transformers import XGLMTokenizer, XGLMForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\nfrom datasets import load_dataset\n","metadata":{"id":"W8JDtdTRB0rL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3,4,5,6,7\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_memory = {i:f\"{int(mem/1024**3)}GB\"for i,mem in enumerate(torch.cuda.mem_get_info())}\nprint(max_memory)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model(base_model:str=\"facebook/xglm-564M\"):\n    tokenizer = XGLMTokenizer.from_pretrained(base_model)\n    model = XGLMForCausalLM.from_pretrained(base_model,device_map='auto',max_memory=max_memory)\n    \n    new_tokens = ['<human>:', '<bot>:']\n    tokenizer.add_tokens(list(new_tokens))\n    model.resize_token_embeddings(len(tokenizer))\n    return model,tokenizer\nmodel,tokenizer = load_model()","metadata":{"id":"Lrtg22NwCC6c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.hf_device_map","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gdown\n\nurl = 'https://drive.google.com/uc?export=download&id=1jbbUtwgwoSQgGnXxzTh-nMReVzEU7ZTU&confirm=t&uuid=d79e2e78-51de-466f-9ceb-3944606141a2&at=AKKF8vwcgi95TGSnSQUNCKx4NTqS:1682865249145'\noutput = 'output.jsonl'\ngdown.download(url, output, quiet=False)","metadata":{"id":"I7gxeJn7CHUf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_prompt(prompt):\n    text = {'prompt':f\"{prompt['Background:']} <human>: {prompt['<human>:']} <bot>: {prompt['<bot>:']}\"\n           }\n    text['token_prompt'] = len(tokenizer.tokenize(text['prompt']))\n    return text\ndef find(prompt):\n    return prompt['token_prompt']<254\ndef preprocess(prompt):\n    inputs = tokenizer(\n        prompt['prompt'],\n        truncation=True,\n        max_length=256,\n        padding=False,\n        return_tensors=None,\n    )\n    inputs['input_ids'].append(tokenizer.eos_token_id)\n    inputs['attention_mask'].append(1)\n    inputs['labels'] = inputs['input_ids'].copy()\n    return inputs","metadata":{"id":"DH0JK5NID4qv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\ndatasets = load_dataset('json',data_files = 'output.jsonl')\ndatasets","metadata":{"id":"-kUNjqBPDxyv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import multiprocessing\ncpu_cores = multiprocessing.cpu_count()\n","metadata":{"id":"4tfXiYT0S-j1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from datasets import Dataset\n# datasets = Dataset.from_dict(datasets['train'][:1_000]) # sample data for test\ndatasets = datasets['train']\ndatasets = datasets.map(format_prompt,remove_columns=['Background:', '<human>:', '<bot>:'],num_proc=cpu_cores)\ndatasets = datasets.filter(find,num_proc=cpu_cores) # two for <\\s> token\ndatasets = datasets.map(preprocess,remove_columns=['prompt','token_prompt'],num_proc=cpu_cores) \nprint(datasets)","metadata":{"id":"NPYyqRNnG7_S","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"U7D2cstUPbyq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\nmicro_batch_size = 4\ngradient_accumulation_steps = batch_size // micro_batch_size\nnum_epochs = 3\nlearning_rate = 3e-7\noutput_dir = 'checkpoint-xglm'","metadata":{"id":"k3s_B_zsqzf0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"WANDB_PROJECT\"] = 'wandb_project'\nos.environ[\"WANDB_LOG_MODEL\"] = 'true'","metadata":{"id":"ntzcUSCZyVg8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_api = user_secrets.get_secret(\"wandb_api\")\n\nwandb.login(key=wandb_api)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_args = TrainingArguments( # สร้าง class train-args\n            per_device_train_batch_size=micro_batch_size, # btch_size \n            gradient_accumulation_steps=gradient_accumulation_steps, # https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation เหมือนจะ ค่อยๆคำนวนค่า gradient ตามค่าที่ใส่เข้าไปรอบ แล้วค่อยปรับ weight ทีเดียว ไม่รู้_\n            gradient_checkpointing=True,\n            warmup_steps=100,# ไม่รู้_\n            num_train_epochs=num_epochs, # จำนวน epoch\n            learning_rate=learning_rate,# ค่า learning-rate\n            fp16= True, # ไม่รู้ว่าคืออะไร ตอนแรก default คือ True เลยเปลี่ยนเป็น False แทน แล้วรันได้เฉย _   ///////// เพราะเราเซ้ตข้างบนไว้ว่าเป็น torch.float16 \n            logging_steps=gradient_accumulation_steps, # แสดงผลตอนเทรนทุกๆ n step gradient descent\n            optim=\"adamw_torch\",# ชื่อ optimizer มั้ง_ /////// yes!!\n            save_strategy=\"steps\", # ไม่รู้_ //////////////////////////////// save model based on epoch? steps?\n            save_steps=1_000, # ไม่รู้_ ///////////////////// Save model every 200 optimizer.step()\n            output_dir=output_dir, # ไม่รู้_ ////////////////////////// Where to save model\n            save_total_limit=3, # ไม่รู้_ /////////////////////////// Limit model save amount (Not to have 300 model file when you train 300 epoch)\n            report_to=\"wandb\", # ใช้ wandb\n            run_name='finetune-xglm', # ชื่อ task\n        )\nprint({i:f\"{mem/1024**3}GB\"for i,mem in enumerate(torch.cuda.mem_get_info())})\n\ntrainer = Trainer(\n      model=model,# model ที่จะเอาไปเทรน\n      train_dataset=datasets, # data ใน train-set\n      # eval_dataset=val_data,\n      args=train_args,\n      data_collator=DataCollatorForSeq2Seq(\n          tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True \n      ),\n  )\nprint({i:f\"{mem/1024**3}GB\"for i,mem in enumerate(torch.cuda.mem_get_info())})\nmodel.config.use_cache = False\nif torch.__version__ >= \"2\" and sys.platform != \"win32\":\n    model = torch.compile(model)\n# train-ai ปกติ\nprint({i:f\"{mem/1024**3}GB\"for i,mem in enumerate(torch.cuda.mem_get_info())})\ntrainer.train()\n\nmodel.save_pretrained('checkpoint')","metadata":{"id":"-snNulttqcrd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"dHn0O1UcVL_T"},"execution_count":null,"outputs":[]}]}