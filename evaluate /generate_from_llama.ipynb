{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOHwl6UpWIEnEf2yDr4JdLW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ohmreborn/question-generation-AIB2023/blob/main/evaluate%20/generate_from_llama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUxhBTRsZSYh"
      },
      "outputs": [],
      "source": [
        "package = \"\"\"\n",
        "appdirs\n",
        "loralib\n",
        "bitsandbytes\n",
        "black\n",
        "black[jupyter]\n",
        "datasets\n",
        "fire\n",
        "peft==0.3.0\n",
        "transformers==4.28.1\n",
        "sentencepiece\n",
        "gradio\n",
        "gdown\n",
        "\"\"\"\n",
        "with open('requirements.txt','w') as f:\n",
        "    f.write(package)\n",
        "  \n",
        "!pip install -r requirements.txt\n",
        "!pip install accelerate==0.18.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/tloen/alpaca-lora/main/templates/alpaca.json"
      ],
      "metadata": {
        "id": "Lfni6f4_ZyHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "A dedicated helper to manage templates and prompt building.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os.path as osp\n",
        "from typing import Union\n",
        "\n",
        "\n",
        "class Prompter(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        \n",
        "        with open('alpaca.json') as fp:\n",
        "            self.template = json.load(fp)\n",
        "\n",
        "    def generate_prompt(\n",
        "        self,\n",
        "        instruction: str,\n",
        "        input: Union[None, str] = None,\n",
        "        label: Union[None, str] = None,\n",
        "    ) -> str:\n",
        "        # returns the full prompt from instruction and optional input\n",
        "        # if a label (=response, =output) is provided, it's also appended.\n",
        "        if input:\n",
        "            res = self.template[\"prompt_input\"].format(\n",
        "                instruction=instruction, input=input\n",
        "            )\n",
        "        else:\n",
        "            res = self.template[\"prompt_no_input\"].format(\n",
        "                instruction=instruction\n",
        "            )\n",
        "        if label:\n",
        "            res = f\"{res}{label}\"\n",
        "        return res\n",
        "\n",
        "    def get_response(self, output: str) -> str:\n",
        "        return output.split(self.template[\"response_split\"])[1].strip()"
      ],
      "metadata": {
        "id": "Y2lywYYkZ1nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "url = \"https://drive.google.com/uc?export=download&id=1BsT2l8e00ZZM-Q1RcVUxWimZ3_rw7dXp\"\n",
        "\n",
        "# https://drive.google.com/uc?export=download&id=14DeJ5Gyl02CcUFE2VSIhHYkTMneBXKQv\n",
        "output = 'adapter_config.json'\n",
        "gdown.download(url, output, quiet=False)\n",
        "url = 'https://drive.google.com/uc?export=download&id=1ErWZE4R_0zZjydVsnAQ7apPUhM31GuO6'\n",
        "# url = 'https://drive.google.com/uc?export=download&id=104yNcI4vE4SjGkvo2VnmnxDpZKTz0P44'\n",
        "output = 'adapter_model.bin'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "id": "GvtDNkhXZ3W-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir checkpoint\n",
        "import shutil\n",
        "\n",
        "path = 'adapter_model.bin'\n",
        "destination = 'checkpoint/adapter_model.bin'\n",
        "dest = shutil.move(path, destination)\n",
        "\n",
        "path = 'adapter_config.json'\n",
        "destination = 'checkpoint/adapter_config.json'\n",
        "dest = shutil.move(path, destination)"
      ],
      "metadata": {
        "id": "WxDu0gvSbias"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from peft import PeftModel\n",
        "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main(\n",
        "    load_8bit: bool = True,\n",
        "    base_model: str = \"decapoda-research/llama-7b-hf\",\n",
        "    lora_weights: str = \"/content/checkpoint\",\n",
        "):\n",
        "    base_model = base_model or os.environ.get(\"BASE_MODEL\", \"\")\n",
        "    \n",
        "    assert (\n",
        "        base_model\n",
        "    ), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n",
        "\n",
        "    tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
        "    \n",
        "    # max_memory = {i:f\"{int(mem/1024**3)}GB\"for i,mem in enumerate(torch.cuda.mem_get_info())}\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        load_in_8bit=load_8bit,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        # max_memory=max_memory\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(\n",
        "        model,\n",
        "        lora_weights,\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "\n",
        "    \n",
        "\n",
        "    # unwind broken decapoda-research config\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
        "    model.config.bos_token_id = 1\n",
        "    model.config.eos_token_id = 2\n",
        "\n",
        "    if not load_8bit:\n",
        "        model.half()  # seems to fix bugs for some users.\n",
        "\n",
        "    model.eval()\n",
        "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
        "        model = torch.compile(model)\n",
        "    return model,tokenizer\n",
        "# \"tloen/alpaca-lora-7b\"\n",
        "model,tokenizer = main(base_model='decapoda-research/llama-7b-hf')\n",
        "print(model)"
      ],
      "metadata": {
        "id": "0QNXtygsZ53s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')\n",
        "device"
      ],
      "metadata": {
        "id": "VEhgO87XZ9WJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for p in model.parameters():\n",
        "  print(p)\n",
        "  break"
      ],
      "metadata": {
        "id": "jkqwNAEc2jbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate(\n",
        "#     instruction,\n",
        "#     input=None,\n",
        "#     temperature=0.5, # ทำให้ model มั่นใจมากขึ้นใน softmax function https://stackoverflow.com/questions/58764619/why-should-we-use-temperature-in-softmax/63471046#63471046\n",
        "#     top_p=0.90, # จะ เอา ค่าความน่าจะเป็นของ top ความน่าจะเป็นที่มากที่สุดมารวมกันจนมากกว่า 0.95 แล้วค่อยให้ model สุ่ม ออกมาhttps://www.linkedin.com/pulse/text-generation-temperature-top-p-sampling-gpt-models-selvakumar\n",
        "#     top_k=10, # เอา 50 แรก แต่ถ้า ใส่ค่า top p ไปด้วย จะทำให้ คิดของ top p ก่อน เช่น ถ้า 50 ตัวแรกมีความน่าจะเป็นรวมกัน = 0.90 ซึ่งไม่ถึงค่าที่ตั้งไว้ก็เอามาไว้ใช้สำหรับการทำนายครั้งถัดไป https://docs.cohere.com/docs/controlling-generation-with-top-k-top-p#2-pick-from-amongst-the-top-tokens-top-k\n",
        "#     repetition_penalty=2.0, # https://arxiv.org/pdf/1909.05858.pdf หน้าที่ 5\n",
        "#     max_new_tokens=1024,\n",
        "#     model=None,\n",
        "#     tokenizer=None,\n",
        "# ):"
      ],
      "metadata": {
        "id": "kHRLY3Vz4aik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(\n",
        "    instruction,\n",
        "    input=None,\n",
        "    temperature=0.75, # ทำให้ model มั่นใจมากขึ้นใน softmax function https://stackoverflow.com/questions/58764619/why-should-we-use-temperature-in-softmax/63471046#63471046\n",
        "    top_p=0.95, # จะ เอา ค่าความน่าจะเป็นของ top ความน่าจะเป็นที่มากที่สุดมารวมกันจนมากกว่า 0.95 แล้วค่อยให้ model สุ่ม ออกมาhttps://www.linkedin.com/pulse/text-generation-temperature-top-p-sampling-gpt-models-selvakumar\n",
        "    top_k=50, # เอา 50 แรก แต่ถ้า ใส่ค่า top p ไปด้วย จะทำให้ คิดของ top p ก่อน เช่น ถ้า 50 ตัวแรกมีความน่าจะเป็นรวมกัน = 0.90 ซึ่งไม่ถึงค่าที่ตั้งไว้ก็เอามาไว้ใช้สำหรับการทำนายครั้งถัดไป https://docs.cohere.com/docs/controlling-generation-with-top-k-top-p#2-pick-from-amongst-the-top-tokens-top-k\n",
        "    repetition_penalty=1.2, # https://arxiv.org/pdf/1909.05858.pdf หน้าที่ 5\n",
        "    max_new_tokens=1024,\n",
        "    model=None,\n",
        "    tokenizer=None,\n",
        "):\n",
        "    prompter = Prompter()\n",
        "    prompt = prompter.generate_prompt(instruction, input,)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        repetition_penalty=repetition_penalty\n",
        "#         **kwargs,\n",
        "    )\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generation_output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            generation_config=generation_config,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "        )\n",
        "    s = generation_output.sequences[0]\n",
        "    output = tokenizer.decode(s)\n",
        "    return prompter.get_response(output)\n",
        "\n"
      ],
      "metadata": {
        "id": "J5G8cdNxaBvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Please create an inference question in the style of TOEFL reading comprehension section. Also provide an answer in the format\""
      ],
      "metadata": {
        "id": "SL3kG4ycaEat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"\"\"\n",
        "The Zaporizhzhia Nuclear Power Plant (NPP) has been reconnected to the Ukrainian power grid, national energy company Ukrenergo said in a statement Monday.\n",
        "\n",
        "Earlier Monday, Ukrenergo said the plant – currently occupied by Russian forces – had been cut off from the grid after the high-voltage line that supplies it was one of those damaged by a Russian attack on the Dnipro region, to the north of Zaporizhzhia, in the early hours of the morning. \n",
        "\n",
        "“As a result of damage to the high-voltage line, the Zaporizhzhia nuclear power plant lost power from the power system and operated from diesel generators,” it said on Telegram. \n",
        "\n",
        "“This is the seventh time since the temporary occupation of the Zaporizhzhia NPP that the Russians have created a nuclear and radiation hazard in the NPP area. Ukrenergo made maximum efforts and restored power to the nuclear plant from the Ukrainian power system,” the statement said.\n",
        "Military and infrastructure facilities in Dnipro were attacked by Russian missiles and drones early on Monday, according to a Telegram post by the Ukrainian air force.\n",
        "\"\"\"\n",
        "res = evaluate(instruction=instruction,input=input,model=model,tokenizer=tokenizer)\n",
        "print(res)"
      ],
      "metadata": {
        "id": "7qxHUNYEaE3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/ohmreborn/question-generation-AIB2023/main/evaluate/eval.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "hHd-oweK2vC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for context in df.iloc[:,0]:\n",
        "  res = evaluate(instruction=instruction,input=context,model=model,tokenizer=tokenizer)\n",
        "  print(res)\n",
        "  print('-----------------------------------')"
      ],
      "metadata": {
        "id": "EwxOk3tDdASe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = df.iloc[0,0]\n",
        "res = evaluate(instruction=instruction,input=input,model=model,tokenizer=tokenizer)\n",
        "print(res)"
      ],
      "metadata": {
        "id": "vNOCBp9yclfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bZ0_klrVcwT5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}