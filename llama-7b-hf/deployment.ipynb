{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOaEJfMKWJWoC27XwCua+nX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ohmreborn/question-generation-AIB2023/blob/main/llama-7b-hf/deployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUxhBTRsZSYh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ติดตั้ง Package ที่ต้องใช้\n",
        "!pip install -q loralib==0.1.1\n",
        "!pip install -q bitsandbytes==0.39.0\n",
        "!pip install -q datasets==2.12.0\n",
        "!pip install -q peft==0.3.0\n",
        "!pip install -q transformers==4.28.1\n",
        "!pip install -q sentencepiece==0.1.99\n",
        "!pip install -q gradio==3.33.1\n",
        "!pip install -q accelerate==0.19.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title โหลด โมเดล\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from peft import PeftModel\n",
        "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
        "\n",
        "\n",
        "def main(\n",
        "    load_8bit: bool = True,\n",
        "    base_model: str = \"decapoda-research/llama-7b-hf\",\n",
        "    lora_weights: str = \"ohmreborn/llama-lora-7b\",\n",
        "):\n",
        "    tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
        "    \n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        load_in_8bit=load_8bit,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(\n",
        "        model,\n",
        "        lora_weights,\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id = 0 \n",
        "    model.config.bos_token_id = 1\n",
        "    model.config.eos_token_id = 2\n",
        "\n",
        "    model.eval()\n",
        "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
        "        model = torch.compile(model)\n",
        "    return model,tokenizer\n",
        "base_model='decapoda-research/llama-7b-hf' #@param {type:\"string\"}\n",
        "model,tokenizer = main(base_model=base_model)\n",
        "device = torch.device('cuda')"
      ],
      "metadata": {
        "id": "0QNXtygsZ53s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title สร้างฟังก์ชันสำหรับ generate ออกมา\n",
        "from typing import Union\n",
        "import requests\n",
        "\n",
        "class Prompter(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        \n",
        "        url = \"https://raw.githubusercontent.com/tloen/alpaca-lora/main/templates/alpaca.json\"\n",
        "        response = requests.request(\"GET\", url)\n",
        "        self.template = response.json()\n",
        "\n",
        "    def generate_prompt(\n",
        "        self,\n",
        "        instruction: str,\n",
        "        input: Union[None, str] = None,\n",
        "        label: Union[None, str] = None,\n",
        "    ) -> str:\n",
        "        if input:\n",
        "            res = self.template[\"prompt_input\"].format(\n",
        "                instruction=instruction, input=input\n",
        "            )\n",
        "        else:\n",
        "            res = self.template[\"prompt_no_input\"].format(\n",
        "                instruction=instruction\n",
        "            )\n",
        "        if label:\n",
        "            res = f\"{res}{label}\"\n",
        "        return res\n",
        "\n",
        "    def get_response(self, output: str) -> str:\n",
        "        return output.split(self.template[\"response_split\"])[1].strip()\n",
        "\n",
        "def generate(\n",
        "    input=None,\n",
        "    temperature=0.75, \n",
        "    top_p=0.95, # จะ เอา ค่าความน่าจะเป็นของ top ความน่าจะเป็นที่มากที่สุดมารวมกันจนมากกว่า 0.95 แล้วค่อยให้ model สุ่ม ออกมาhttps://www.linkedin.com/pulse/text-generation-temperature-top-p-sampling-gpt-models-selvakumar\n",
        "    top_k=50, # เอา 50 แรก แต่ถ้า ใส่ค่า top p ไปด้วย จะทำให้ คิดของ top p ก่อน เช่น ถ้า 50 ตัวแรกมีความน่าจะเป็นรวมกัน = 0.90 ซึ่งไม่ถึงค่าที่ตั้งไว้ก็เอามาไว้ใช้สำหรับการทำนายครั้งถัดไป https://docs.cohere.com/docs/controlling-generation-with-top-k-top-p#2-pick-from-amongst-the-top-tokens-top-k\n",
        "    max_new_tokens=1024,\n",
        "    instruction=\"Please create an inference question in the style of TOEFL reading comprehension section. Also provide an answer in the format\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "):\n",
        "    prompter = Prompter()\n",
        "    prompt = prompter.generate_prompt(instruction, input,)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        repetition_penalty=1.2\n",
        "    )\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generation_output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            generation_config=generation_config,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "        )\n",
        "    s = generation_output.sequences[0]\n",
        "    output = tokenizer.decode(s)\n",
        "    return prompter.get_response(output)\n",
        "\n"
      ],
      "metadata": {
        "id": "J5G8cdNxaBvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate(input=example_1))"
      ],
      "metadata": {
        "id": "eCuEY4NE3Bfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run app\n",
        "import gradio as gr\n",
        "example_1 = \"\"\"Education is the process of facilitating learning, or the acquisition of knowledge, skills, values, morals, beliefs, habits, and personal development. There are many types of potential educational aims and objectives, irrespective of the specific subject being learned. Some can cross multiple school disciplines.\"\"\",\n",
        "example_2 = \"\"\"History – discovery, collection, organization, and presentation of information about past events. History can also mean the period of time after writing was invented (the beginning of recorded history).\"\"\",\n",
        "example_3 = \"\"\"Culture – a set of patterns of human activity within a community or social group and the symbolic structures that give significance to such activity. Customs, laws, dress, architectural style, social standards, and traditions are all examples of cultural elements. Since 2010, Culture is considered the Fourth Pillar of Sustainable Development by UNESCO.\"\"\",\n",
        "example_4 = \"\"\"Health sciences are those sciences which focus on health, or health care, as core parts of their subject matter. Health sciences relate to multiple academic disciplines, including STEM disciplines and emerging patient safety disciplines.\"\"\"\n",
        "demo = gr.Interface(fn=generate,\n",
        "                    inputs=[gr.Textbox(value=example_1),\n",
        "                            gr.Slider(1,1024,value=1024,step=1,label='max_new_tokens'),\n",
        "                            gr.Slider(0.05,1,value=0.75,step=0.05,label='temperature'),\n",
        "                            gr.Slider(0.05,1,value=0.95,step=0.05,label='top_p'),\n",
        "                            gr.Slider(1,100,value=65,step=1,label='top_k')], \n",
        "                    outputs=[\"text\"],\n",
        "                    examples = [[example_1,512,0.75,0.95,65],[example_2,512,0.75,0.95,60],[example_3,512,0.75,0.95,50],[example_4,512,0.75,0.95,45]]\n",
        "                    )\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "bZ0_klrVcwT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S6i-hR-31Iw3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}