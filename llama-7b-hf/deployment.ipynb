{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNm7zMeLLWYX4OKD02jdoIU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ohmreborn/question-generation-AIB2023/blob/main/llama-7b-hf/deployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUxhBTRsZSYh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ติดตั้ง Package ที่ต้องใช้\n",
        "!pip install -q loralib==0.1.1\n",
        "!pip install -q bitsandbytes==0.39.0\n",
        "!pip install -q datasets==2.12.0\n",
        "!pip install -q peft==0.3.0\n",
        "!pip install -q transformers==4.28.1\n",
        "!pip install -q sentencepiece==0.1.99\n",
        "!pip install -q gradio==3.33.1\n",
        "!pip install -q gdown==4.6.6\n",
        "!pip install -q accelerate==0.19.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title โหลด โมเดล\n",
        "!mkdir checkpoint\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from peft import PeftModel\n",
        "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
        "import gdown\n",
        "import shutil\n",
        "url = \"https://drive.google.com/uc?export=download&id=1BsT2l8e00ZZM-Q1RcVUxWimZ3_rw7dXp\"\n",
        "\n",
        "output = 'adapter_config.json'\n",
        "gdown.download(url, output, quiet=False)\n",
        "url = 'https://drive.google.com/uc?export=download&id=1ErWZE4R_0zZjydVsnAQ7apPUhM31GuO6'\n",
        "output = 'adapter_model.bin'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "path = 'adapter_model.bin'\n",
        "destination = 'checkpoint/adapter_model.bin'\n",
        "dest = shutil.move(path, destination)\n",
        "\n",
        "path = 'adapter_config.json'\n",
        "destination = 'checkpoint/adapter_config.json'\n",
        "dest = shutil.move(path, destination)\n",
        "\n",
        "\n",
        "def main(\n",
        "    load_8bit: bool = True,\n",
        "    base_model: str = \"decapoda-research/llama-7b-hf\",\n",
        "    lora_weights: str = \"/content/checkpoint\",\n",
        "):\n",
        "    base_model = base_model or os.environ.get(\"BASE_MODEL\", \"\")\n",
        "\n",
        "    tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
        "    \n",
        "    # max_memory = {i:f\"{int(mem/1024**3)}GB\"for i,mem in enumerate(torch.cuda.mem_get_info())}\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        load_in_8bit=load_8bit,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        # max_memory=max_memory\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(\n",
        "        model,\n",
        "        lora_weights,\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "\n",
        "    \n",
        "\n",
        "    # unwind broken decapoda-research config\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
        "    model.config.bos_token_id = 1\n",
        "    model.config.eos_token_id = 2\n",
        "\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
        "        model = torch.compile(model)\n",
        "    return model,tokenizer\n",
        "base_model='decapoda-research/llama-7b-hf' #@param {type:\"string\"}\n",
        "model,tokenizer = main(base_model=base_model)\n",
        "device = torch.device('cuda')"
      ],
      "metadata": {
        "id": "0QNXtygsZ53s",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title สร้างฟังก์ชันสำหรับ generate ออกมา\n",
        "from typing import Union\n",
        "import requests\n",
        "\n",
        "class Prompter(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        \n",
        "        url = \"https://raw.githubusercontent.com/tloen/alpaca-lora/main/templates/alpaca.json\"\n",
        "        response = requests.request(\"GET\", url)\n",
        "        self.template = response.json()\n",
        "\n",
        "    def generate_prompt(\n",
        "        self,\n",
        "        instruction: str,\n",
        "        input: Union[None, str] = None,\n",
        "        label: Union[None, str] = None,\n",
        "    ) -> str:\n",
        "        # returns the full prompt from instruction and optional input\n",
        "        # if a label (=response, =output) is provided, it's also appended.\n",
        "        if input:\n",
        "            res = self.template[\"prompt_input\"].format(\n",
        "                instruction=instruction, input=input\n",
        "            )\n",
        "        else:\n",
        "            res = self.template[\"prompt_no_input\"].format(\n",
        "                instruction=instruction\n",
        "            )\n",
        "        if label:\n",
        "            res = f\"{res}{label}\"\n",
        "        return res\n",
        "\n",
        "    def get_response(self, output: str) -> str:\n",
        "        return output.split(self.template[\"response_split\"])[1].strip()\n",
        "\n",
        "def evaluate(\n",
        "    input=None,\n",
        "    instruction=\"Please create an inference question in the style of TOEFL reading comprehension section. Also provide an answer in the format\",\n",
        "    temperature=0.75, # ทำให้ model มั่นใจมากขึ้นใน softmax function https://stackoverflow.com/questions/58764619/why-should-we-use-temperature-in-softmax/63471046#63471046\n",
        "    top_p=0.95, # จะ เอา ค่าความน่าจะเป็นของ top ความน่าจะเป็นที่มากที่สุดมารวมกันจนมากกว่า 0.95 แล้วค่อยให้ model สุ่ม ออกมาhttps://www.linkedin.com/pulse/text-generation-temperature-top-p-sampling-gpt-models-selvakumar\n",
        "    top_k=50, # เอา 50 แรก แต่ถ้า ใส่ค่า top p ไปด้วย จะทำให้ คิดของ top p ก่อน เช่น ถ้า 50 ตัวแรกมีความน่าจะเป็นรวมกัน = 0.90 ซึ่งไม่ถึงค่าที่ตั้งไว้ก็เอามาไว้ใช้สำหรับการทำนายครั้งถัดไป https://docs.cohere.com/docs/controlling-generation-with-top-k-top-p#2-pick-from-amongst-the-top-tokens-top-k\n",
        "    repetition_penalty=1.2, # https://arxiv.org/pdf/1909.05858.pdf หน้าที่ 5\n",
        "    max_new_tokens=1024,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "):\n",
        "    prompter = Prompter()\n",
        "    prompt = prompter.generate_prompt(instruction, input,)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        repetition_penalty=repetition_penalty\n",
        "    )\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generation_output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            generation_config=generation_config,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "        )\n",
        "    s = generation_output.sequences[0]\n",
        "    output = tokenizer.decode(s)\n",
        "    return prompter.get_response(output)\n",
        "\n"
      ],
      "metadata": {
        "id": "J5G8cdNxaBvX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sample input"
      ],
      "metadata": {
        "id": "dXfgCRDQAlAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Nutrition is the biochemical and physiological process by which an organism uses food to support its life. \n",
        "It provides organisms with nutrients, which can be metabolized to create energy and chemical structures. \n",
        "Failure to obtain sufficient nutrients causes malnutrition. \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "51qC20P6AkGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run app\n",
        "import gradio as gr\n",
        "demo = gr.Interface(fn=evaluate, inputs=\"text\", outputs=\"text\")\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "bZ0_klrVcwT5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 30 วิ"
      ],
      "metadata": {
        "id": "-n9Nxvxu5bJE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}